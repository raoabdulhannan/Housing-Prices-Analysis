---
title: "Predicting Effect of Distance from Rail Trails on House Prices"
author: "Rao Abdul Hannan, Mark Ma"
format:
  html:
    colorlinks: true
    message: false
execute:
  echo: false
---

```{r, echo=FALSE}
suppressMessages(library(tidyverse))
suppressMessages(library(GGally))
suppressMessages(library(car))
suppressMessages(library(gt))
suppressMessages(library(scales))
suppressMessages(library(ggdag))
suppressMessages(library(patchwork))
```

```{r, echo=FALSE}
rail <- read.csv("rail.csv")
```

# Executive Summary

This report analyzes home value trends from 1998 to 2014, concentrating on properties with less than 0.56 acres of land. The primary objective was to identify the key factors that drive the appreciation of home values within this specific segment of the housing market. To achieve this, we employed statistical methods such as regression analysis to explore the relationships between home value increases and various potential factors, including location, property size, and proximity to amenities like schools, parks, and shopping centers. Additionally, we considered the impact of the broader economic conditions during the study period on property values. We found that the closer a home is to the rail trail, the more its value increases. Specifically, for every foot nearer to the trail, a home's price goes up by about 0.05%. This effect is stronger than that of property size — while larger homes do sell for more, proximity to the trail has a bigger impact. By excluding two homes that experienced over \$500,000 in value increases due to major renovations, we ensured that the results reflect typical market behavior rather than being skewed by exceptional cases. This approach provides a clearer understanding of the primary drivers behind home value increases for properties within the specified land size.

# Introduction

During the late 19th and early 20th centuries, the United States saw the construction of an extensive network of rail lines that connected towns and cities, facilitating passenger travel and cargo transport. However, with the advent of the automobile and the expansion of the Interstate Highway System, reliance on rail transportation diminished significantly. This shift led to the closure and abandonment of many rail lines; some were preserved for potential future use, while others were sold.

Starting in the 1980s, a transformative initiative began to re-purpose these defunct rail lines into rail trails—dedicated walking and biking paths that trace the routes of the old tracks. Characterized by their long, continuous stretches and gentle gradients (a legacy of trains' inability to navigate steep inclines), these trails are often paved and highly accessible, making them ideal for recreational cycling and walking.

The emergence of rail trails has sparked interest in their potential impact on residential property values. It is hypothesized that these trails enhance the attractiveness of nearby homes, with buyers possibly willing to pay a premium for the convenience of easy access to recreational and commuting options.

Acme Homes, LLC, a company specializing in large-scale residential developments, is exploring opportunities to maximize the profitability of their future projects. The development manager, Mr. W. E. Coyote, has commissioned this report to investigate the following key questions:

**- Are rail trails appealing to home buyers to the extent that they increase the willingness to pay for houses located nearer to them?**

**- If they are, what is the specific relationship between a property's proximity to a rail trail and its market value?**

This report aims to analyze these questions by examining housing market data in relation to the proximity of homes to rail trails. The findings will assist Acme Homes in making informed decisions about where to focus their development efforts to achieve optimal returns.

# Exploratory Data Analysis

This study utilizes the `rail` data set containing information of 104 houses in the Northampton (01060) and Florence (01060) neighborhoods in Northampton, Massachusetts from an observational study. The details of the variables are appended below:

```{r, echo=FALSE}
rail_vars <- data.frame(Variable = c("housenum", 
  "price1998_adj", 
  "price2007_adj", 
  "price2011_adj", 
  "price1998", 
  "price2007", 
  "price2011", 
  "price2014", 
  "distance", 
  "acre", 
  "bedrooms", 
  "bikescore", 
  "walkscore", 
  "garage_spaces", 
  "latitude", 
  "longitude", 
  "squarefeet", 
  "streetname", 
  "streetno", 
  "zip"),
Description = c("A unique number for each house",
                         "Zillow's estimated value for the home in 1998, in thousands of 2014 dollars",
                         "Zillow's estimated value for the home in 2007, in thousands of 2014 dollars",
                         "Zillow's estimated value for the home in 2011, in thousands of 2014 dollars",
                         "Zillow's estimated value for the home in 1998, in thousands of dollars",
                         "Zillow's estimated value for the home in 2007, in thousands of dollars",
                         "Zillow's estimated value for the home in 2011, in thousands of dollars",
                         "Zillow's estimated value for the home in 2014, in thousands of dollars",
                         "Distance (feet) to the nearest entry to the rail trail network",
                         "Number of acres of property",
                         "How many bedrooms the home has",
                         "Bike friendliness of the area, estimated by WalkScore.com. 0-100 scale, where 100 indicates high bike-frinedliness, such as flat terrain and good bike lanes.",
                         "Walkability of the area, estimated by WalkScore.com. 0-100 scale, where 100 indicates high walkability, so most daily tasks can be done without a car",
                         "Number of garage parking spaces (0-4)",
                         "House's latitude",
                         "House's longitude",
                         "Square footage of the home's interior finished space (in thousands of square feet)",
                         "Name of the street the house is on",
                         "House number on the street",
                         "ZIP code of the house (leading 0 omitted). 1060 is Northampton, MA; 1062 is Florence, MA."))
```

```{r, echo=FALSE}
rail_vars |>
  gt() |>
  cols_label(
    Variable = "Variable Name",
    Description = "Variable Description"
  ) |>
  fmt_markdown(columns = c(Description)) |>
  cols_width(
    Variable ~ px(150),
    Description ~ px(300)
  ) |>
  tab_options(
    table.width = pct(100),
    column_labels.border.bottom.style = "solid",
    column_labels.border.bottom.width = px(2)
  ) |>
  text_transform(
    locations = cells_body(columns = c(Description)),
    fn = function(x) {
      stringr::str_wrap(x, width = 50)
    }
  ) |>
  tab_footnote(footnote = md("**Table 1**: Variable Descriptions")) |>
  tab_style(style = list(cell_text(align = "center")),
            location = cells_footnotes()) |>
  tab_options(
    column_labels.background.color = "steelblue3"
  )
```

The variable of interest in this case is `price2014` and how it is affected by the `distance` variable. First and foremost, we checked the distribution of `price2014` by the aid of a histogram, depicted in Figure 1.

```{r, echo=FALSE}
price_hist <- rail |>
  ggplot(aes(x = price2014)) +
  geom_histogram(color = "gray10", fill = "steelblue3") +
  theme_light() +
  scale_x_continuous(label = label_dollar()) +
  labs(x = "House Price in 2014 (x 100K)", y = "Count",
       title = expression(bold("Figure 1") * ": Distribution of 2014 House Prices")) +
  theme(axis.title = element_text(size = 10, hjust = 0.5),
        plot.title = element_text(size = 12, hjust = 0.5)) +
  annotate("rect",
           xmin = 820, xmax = 920,
           ymin = -1, ymax = 3,
           color = "tomato", fill = NA) +
  annotate("text", x = 820, y = 8,
           label = "Outlier", size = 4, color = "gray10", hjust = -0.3) +
  annotate("segment", x = 870, xend = 870,
           y = 3, yend = 7, color = "gray10",
           arrow = arrow(type = "closed", length = unit(0.1, "inches")))
  

suppressMessages({
  print(price_hist)
})
```

It is quite evident that there is an outlier with an unusually high price, as highlighted in Figure 1. Upon further inspection, this is house #97 in the data set which has 6 bedrooms and a price of $\$879,000$ however, its values for other variables including `acre, bikescore, walkscore, distance, garage_space` and `squarefeet` are not such that they would suggest an incredibly high price like the one we are observing in the data set. This could potentially cause problems when we fit a model on the data since the outlier may pull the regression function towards it and adversely affect the slope of the estimated regression line.

Next, we critically evaluate `price2014` against all our potential covariates to check the behavior of the data and decide which variables we need to include in the regression model.

## Key takeaways

```{r}
rail |>
  dplyr::select(acre, price2014) |>
  ggplot(aes(x = acre, y = price2014)) +
  geom_point(color = "steelblue3", alpha = 0.7) +
  scale_y_continuous(label = label_dollar()) +
  theme_light() +
  labs(x = "Acre", y = "House Price in 2014",
       title = expression(bold("Figure 2") * ": House Price in 2014 vs Acre")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5),
        axis.title = element_text(size = 10, hjust = 0.5))
```


-   `acre`: There is no evident trend in the `price2014` vs `acre` plot displayed in Figure 2. However, underlying trends can sometimes be invisible in plots and we strongly believe that the size of the property should effect the price of the house. Therefore, we decide to include the `acre` variable as a covariate in our regression model

```{r}
rail |>
  dplyr::select(bedrooms, price2014) |>
  ggplot(aes(x = bedrooms, y = price2014)) +
  geom_point(color = "steelblue3", alpha = 0.7) +
  scale_y_continuous(label = label_dollar()) +
  theme_light() +
  labs(x = "No. of Bedrooms", y = "House Price in 2014",
       title = expression(bold("Figure 3") * ": House Price in 2014 vs No. of Bedrooms")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5),
        axis.title = element_text(size = 10, hjust = 0.5))
```


-   `bedrooms`: The price seems to increase with each additional bedroom, which is visible in the Figure 3 and hence we include it as a covariate


```{r}
rail |>
  dplyr::select(bikescore, distance, price2014) |>
  ggpairs(progress = FALSE,
          upper = list(continuous = wrap("cor", color = "steelblue3")),
          lower = list(continuous = wrap("points", alpha = 0.7, color = "steelblue3")),
          diag = list(continuous = wrap("densityDiag", color = "steelblue3"))) +
  theme_light() +
  labs(title = expression(bold("Figure 4") * ": Distance and House Price vs Bikescore")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
```

-   `bikescore`: The `bikescore` variable is effectively calculated through the `distance` variable. A negative non-linear trend is prominent in the `bikescore` vs `distance` plot in Figure 4 with a correlation value of $-0.836$. Since our primary research question is concerned with `distance`, we decide to exclude `bikescore` from our model to avoid issues with multi-collinearity which would lead to higher standard errors for the coefficient of `distance`, resulting in wider confidence intervals which significantly limit our capability to make inference about the effect of `distance` on `price2014`. It is also important to highlight that including `bikescore` in the model will not allow us to capture the full affect of `distance` on the house prices, rather just the direct affect since `bikescore` is a mediator as depicted by the Directed Acylic Graph (DAG) in Figure 5

```{r, echo=FALSE}
set.seed(2)
bikescore_dag <- dagify(B ~ D,
  P ~ B, P ~ D,
  labels = c(
    "B" = "Bikescore",
    "D" = "Distance",
    "P" = "House Price"
  )
)
suppressWarnings({
ggdag(bikescore_dag, text = FALSE, edge_type = "link_arc") +
  geom_dag_node(color = "steelblue3") +
  geom_dag_edges(color = "gray10") +
  geom_dag_label(aes(label = label), vjust = 0, hjust = 0.5, size = 2, fill = "white") +
  theme_dag() +
  labs(title = expression(bold("Figure 5") * ": DAG showing Bikescore as a mediator to Distance")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
})
```
\newpage
-   `distance`: It is the primary covariate of concern and is therefore added in the model


```{r}
rail |>
  dplyr::select(garage_spaces, price2014) |>
  ggplot(aes(x = garage_spaces, y = price2014)) +
  geom_point(color = "steelblue3", alpha = 0.7) +
  scale_y_continuous(label = label_dollar()) +
  theme_light() +
  labs(x = "No. of Garages", y = "House Price in 2014",
       title = expression(bold("Figure 6") * ": House Price in 2014 vs No. of Garages")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5),
        axis.title = element_text(size = 10, hjust = 0.5))
```


-   `garage_spaces`: A house with more garages is expected to have a higher price. This is validated by the `price2014` vs `garage_spaces` plot in Figure 6 where a minor positive trend can be seen, therefore we add it in our model
\newpage
```{r}
plot_sqft <- rail |>
  dplyr::select(squarefeet, price2014) |>
  ggplot(aes(x = squarefeet, y = price2014)) +
  geom_point(color = "steelblue3", alpha = 0.7) +
  geom_smooth(method = "lm", color = "tomato", se = FALSE) +
  scale_y_continuous(label = label_dollar()) +
  theme_light() +
  labs(x = "Square Footage", y = "House Price in 2014",
       title = expression(bold("Figure 7") * ": House Price in 2014 vs Square Footage")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5),
        axis.title = element_text(size = 10, hjust = 0.5))
suppressMessages({
  print(plot_sqft)
})
```


-   `squarefeet`: This variable has a positively linear trend which is eminent in the `price2014` vs `squarefeet` plot in Figure 7. This aligns well with our expectations because a bigger house is generally expected to cost more
\newpage
```{r}
rail |>
  dplyr::select(walkscore, distance, price2014) |>
  ggpairs(progress = FALSE,
          upper = list(continuous = wrap("cor", color = "steelblue3")),
          lower = list(continuous = wrap("points", alpha = 0.7, color = "steelblue3")),
          diag = list(continuous = wrap("densityDiag", color = "steelblue3"))) +
  theme_light() +
  labs(title = expression(bold("Figure 8") * ": Distance and House Price vs Walkscore")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
```


-   `walkscore`: Just like `bikescore`, this variable is also calculated directly through the `distance` variable and has a negative non-linear trend observed in Figure 8 along with a high correlation value of $-0.761$, which leads us to exclude this variable from the model. This variable is also a mediator as displayed in Figure 9


```{r, echo=FALSE}
set.seed(2)
walkscore_dag <- dagify(W ~ D,
  P ~ W, P ~ D,
  labels = c(
    "W" = "Walkscore",
    "D" = "Distance",
    "P" = "House Price"
  )
)
suppressWarnings({
ggdag(walkscore_dag, text = FALSE, edge_type = "link_arc") +
  geom_dag_node(color = "steelblue3") +
  geom_dag_edges(color = "gray10") +
  geom_dag_label(aes(label = label), vjust = 0, hjust = 0.5, size = 2, fill = "white") +
  theme_dag() +
  labs(title = expression(bold("Figure 9") * ": DAG showing Walkscore as a mediator to Distance")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
})
```
\newpage
-   `zip`: This variable gives us information on both `distance` and `price2014` i.e. it is a confounder as depicted in Figure 10.

```{r, echo=FALSE}
set.seed(2)
zip_dag <- dagify(D ~ Z,
  P ~ Z, P ~ D,
  labels = c(
    "Z" = "Zipcode",
    "D" = "Distance",
    "P" = "House Price"
  )
)
suppressWarnings({
ggdag(zip_dag, text = FALSE, edge_type = "link_arc") +
  geom_dag_node(color = "steelblue3") +
  geom_dag_edges(color = "gray10") +
  geom_dag_label(aes(label = label), vjust = 0, hjust = 0.5, size = 2, fill = "white") +
  theme_dag() +
  labs(title = expression(bold("Figure 10") * ": DAG showing Zipcode as a confounder for Distance")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
})
```

The zip code of a house provides approximate information on the location of the house which can help us get an idea of the `distance` value for the house i.e. how far it is from the nearest rail track entry. Similarly, it also provides information on the `price2014` variable because zip codes are associated with schools, hospitals and other facilities which can drive up the price of houses in the vicinity. We verify this by calculating the average distance and house prices for both zip codes and observe substantial differences between the values as shown in Table 2. Excluding a confounder such as `zip` would also lead to biased estimates of the coefficient of `distance` which would raise major concerns about our conclusions.

```{r, echo=FALSE}
# Calculating avg distance for both zip codes
avg_distance <- rail |>
  group_by(zip) |>
  summarize(avg_distance = round(mean(distance), 2))
avg_price <- rail |>
  group_by(zip) |>
  summarize(avg_price = round(mean(price2014), 2))
avg_combined <- avg_distance |>
  left_join(avg_price, by = "zip")
avg_combined |>
  gt() |>
  cols_label(
    zip = "ZIP Code",
    avg_distance = "Avg Distance",
    avg_price = "Avg Price"
  ) |>
  tab_footnote(footnote = md("**Table 2**: Avg Distance and Price by Zip Code")) |>
  tab_style(
    style = list(
      cell_text(align = "center")
      ),
            location = cells_footnotes()
  ) |>
  tab_options(
    column_labels.background.color = "steelblue3"
  )
```
\newpage
-   `latitude`, `longitude`: Both the `latitude` and `longitude` variables provide us information on the location of the house, however we have the variable `zip` which provides us similar information. We exclude `latitude` and `longitude` since they would require us to fit a non-parametric smoother and prefer `zip` over it for location information

-   `streetname`, `streetno`: We again prefer `zip` over these two variables for location since they are discrete and have $73$ and $86$ different values and using them in our regression model would result in us losing an extensive number of degrees of freedom

```{r}
rail |>
  dplyr::select(squarefeet, bedrooms, garage_spaces) |>
  ggpairs(progress = FALSE,
          upper = list(continuous = wrap("cor", color = "steelblue3")),
          lower = list(continuous = wrap("points", alpha = 0.7, color = "steelblue3")),
          diag = list(continuous = wrap("densityDiag", color = "steelblue3"))) +
  theme_light() +
  labs(title = expression(bold("Figure 11") * ": No. of Bedrooms and Garages vs Square Footage")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
```


-   `squarefeet` has a positive association with both `bedrooms` and `garage_spaces` as displayed by the plots and correlation values in Figure 11, which is reasonable since a bigger house is expected to have more bedrooms and garage spaces. This observation will be referenced in the Results section later on

-   There is also a high leverage point which can be seen in all the `price2014` vs covariates plots displayed above. This is identified as house #89, which has a price of $\$513K$ even though it has 6 bedrooms and 4 garage spaces, and a `squarefeet` value greater than $4$. Further investigation on whether this is a bad leverage point will be carried out in the Methods section

# Methods

In the EDA section, we emphasized house #97 being a potential outlier. To cater for this issue, we applied a logarithmic transformation to `price2014` and the resulting distribution can be seen in Figure 13.

```{r, echo=FALSE}
log_price_hist <- rail |>
  ggplot(aes(x = log(price2014))) +
  geom_histogram(color = "gray10", fill = "steelblue3") +
  theme_light() +
  scale_x_continuous(label = label_dollar()) +
  labs(x = "Log of House Price in 2014 (x 100K)", y = "Count",
       title = expression(bold("Figure 13") * ": Distribution of Log of 2014 House Prices")) +
  theme(axis.title = element_text(size = 10, hjust = 0.5),
        plot.title = element_text(size = 14, hjust = 0.5)) +
  annotate("rect",
           xmin = 6.73, xmax = 6.9,
           ymin = -0.5, ymax = 1.5,
           color = "tomato", fill = NA) +
  annotate("text", x = 6.5, y = 5,
           label = "Closer to other points now", size = 4, color = "gray10", hjust = 0.2) +
  annotate("segment", x = 6.81, xend = 6.81,
           y = 1.5, yend = 4.5, color = "gray10",
           arrow = arrow(type = "closed", length = unit(0.1, "inches")))
suppressMessages({
  print(log_price_hist)
})
```

Applying the transformation stabilizes the skewness of `price2014` and therefore, we decide to use this as the response variable for our regression analysis. The regression model we fit is represented by $Eq\; (1)$:

\begin{align}
\begin{split}
E(\log(price2014)) &= {\beta}_0 + {\beta}_1distance + {\beta}_2acre + {\beta}_3bedrooms + {\beta}_4garagespaces \\
&+ {\beta}_5squarefeet + {\beta}_6zip
\end{split}
\end{align}

where all variables are treated as continuous except `zip` which is treated as a discrete variable with two levels i.e. 1060 and 1062 with the following coding:

-   `zip` $= 0;\quad for\; 1060$

-   `zip` $= 1;\quad for\; 1062$

The `bedrooms` variable could also have been coded as a discrete variable however for the sake of interpretability and preserving more degrees of freedom, we include it as a continuous variable. Since `bedrooms` variable has six distinct values, we would need to create five dummy variables to code it as a discrete variable, which would lead to losing five degrees of freedom and also interpreting results for six different categories of houses i.e. one category for each distinct value of `bedrooms`. 

Keeping in view our end goal is to predict the effect of `distance` on `price2014`, we conduct the following test when fitting the regression model:
$$H_0:\quad \beta_1 = 0$$
$$H_A:\quad \beta_1 \neq 0$$
Under the null hypotheses $H_0$, the coefficient of `distance` $\beta_1$ is $0$ i.e. `distance` does not have any effect on `price2014` whereas we will reject the null if we get a $p-value \leq \alpha = 0.05$ which is our significance level. The output of the `summary()` function after fitting the model in `R` is appended below:

```{r, echo=FALSE}
model1 <- lm(log(price2014) ~ distance + acre + bedrooms + garage_spaces + squarefeet + factor(zip), data = rail)
summary_model1 <- summary(model1)
coef_model1 <- as.data.frame(summary_model1$coefficients)
coef_model1 <- coef_model1 |>
  rownames_to_column(var = "Term") |>
  rename(
    Estimate = Estimate,
    StdError = `Std. Error`,
    tValue = `t value`,
    pValue = `Pr(>|t|)`
  )

rse <- summary_model1$sigma
df <- summary_model1$df[2]
r_squared <- summary_model1$r.squared
adj_r_squared <- summary_model1$adj.r.squared
f_statistic <- summary_model1$fstatistic[1]
f_statistic_df <- summary_model1$fstatistic[2:3]

summary_stats <- data.frame(
  Term = c("Residual Standard Error", "Deg of Freedom", "Multiple R-squared", "Adjusted R-squared", "F-statistic"),
  Estimate = c(rse, df, r_squared, adj_r_squared, f_statistic),
  StdError = c(NA, NA, NA, NA, NA),
  tValue = c(NA, NA, NA, NA, NA),
  pValue = c(NA, NA, NA, NA, NA)
)

combined_summary <- bind_rows(coef_model1, summary_stats)

combined_summary |>
  gt() |>
  fmt_number(
    columns = c(Estimate, StdError, tValue, pValue),
    decimals = 5
  ) |>
  fmt("pValue",
      fns = function(x) format.pval(x, digits = 3, epa = 0.001)) |>
  cols_label(
    Term = "Variable",
    Estimate = "Estimate",
    StdError = "Standard Error",
    tValue = "t-value",
    pValue = "p-value"
  ) |>
  cols_width(
    Term ~ px(150),
    everything() ~ px(120)
  ) |>
  tab_footnote(footnote = md("**Table 3**: Output of Linear Regression")) |>
  tab_style(
    style = list(
      cell_text(align = "center")
      ),
            location = cells_footnotes()
  ) |>
  tab_options(
    column_labels.background.color = "steelblue3"
  )
```

To check the goodness-of-fit of our model and identify any potential problems, we turn towards diagnostic plots shown in Figure 13.

```{r, fig.height=10, fig.width=10, echo = FALSE}
y = log(rail$price2014)
y_hat = fitted(model1)
std_residuals <- rstandard(model1)
stud_residuals <- rstudent(model1)
plot_data <- data.frame(
  log_price2014 = y,
  fitted_values = y_hat,
  std_res = std_residuals,
  stu_res = stud_residuals,
  sqrt_std_res = sqrt(abs(std_residuals))
)

plot1 <- plot_data |>
  ggplot(aes(x = y_hat, y = log_price2014)) +
  geom_point(shape = 1, color = "steelblue3") +
  theme_light() +
  labs(x = "Observed Values", y = "Fitted Values",
       title = "Fitted vs Observed Values") +
  theme(plot.title = element_text(size = 12, hjust = 0.5))

plot2 <- plot_data |>
  ggplot(aes(x = y_hat, y = std_res)) +
  geom_point(shape = 1, color = "steelblue3") +
  geom_hline(yintercept = c(-2, 0, 2), linetype = "dashed", color = "tomato") +
  theme_light() +
  labs(x = "Fitted Values", y = "Standardized Residuals",
       title = "Standardized Residuals vs Fitted Values") +
  theme(plot.title = element_text(size = 12, hjust = 0.5))


plot3 <- plot_data |>
  ggplot(aes(x = y_hat, y = sqrt_std_res)) +
  geom_point(shape = 1, color = "steelblue3") +
  theme_light() +
  labs(x = "Fitted Values", y = expression(sqrt("Standardized Residuals")),
       title = expression(sqrt("Standardized Residuals") * " vs Fitted Values")) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))


plot4 <- plot_data |>
  ggplot(aes(x = y_hat, y = stu_res)) +
  geom_point(shape = 1, color = "steelblue3") +
  geom_hline(yintercept = c(-3.3, 0, 3.3), linetype = "dashed", color = "tomato") +
  theme_light() +
  labs(x = "Fitted Values", y = "Studentized Residuals",
       title = "Studentized Residuals vs Fitted Values") +
  theme(plot.title = element_text(size = 12, hjust = 0.5))


n = 104
alpha = 0.05

my_envelope <- function(n, alpha, conf = 1-(alpha/n)) {
  normal <- qnorm((1 + conf) / 2)
  se <- normal * sqrt(1/n + (n-1:n)^2 / (n*(n-1)^2))
  ci_lower <- -se
  ci_upper <- se
  data.frame(ci_lower = ci_lower, ci_upper = ci_upper)
}

my_qqplot <- function(model) {
  std_res <- rstandard(model)
  n <- length(std_res)
  alpha = 0.05
  qq_data <- qqnorm(std_res, plot.it = FALSE)
  envelope <- my_envelope(n, alpha)
  plot_data <- data.frame(
    theoretical_quantiles = qq_data$x,
    observed_quantiles = qq_data$y,
    lower = qq_data$x + envelope$ci_lower,
    upper = qq_data$x + envelope$ci_upper
  )
  plot_data |>
  ggplot(aes(x = theoretical_quantiles, y = observed_quantiles)) +
    geom_point(shape = 1, color = "steelblue3") +
    geom_abline(intercept = 0, slope = 1, color = "tomato") +
    geom_ribbon(aes(ymin = lower, ymax = upper),
                fill = "gray", alpha = 0.5) +
    labs(x = "Theoretical Quantiles", y = "Standardized Residuals",
         title = "Q-Q Plot") +
    theme_light() +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
}

plot5 <- my_qqplot(model1)

# plot5 <- plot_data |>
#   ggplot(aes(sample = std_res)) +
#   stat_qq(color = "steelblue3", shape = 1) +
#   stat_qq_line(color = "tomato") +
#   theme_light() +
#   labs(x = "Theoretical Quantiles", y = "Standarized Residuals")


model1_resiuals <- rstandard(model1)
leverage <- hatvalues(model1)
cooks_distance <- cooks.distance(model1)
model_data <- data.frame(
  index = seq_along(model1_resiuals),
  resids = model1_resiuals,
  leverage = leverage,
  cooks_distance = cooks_distance,
  high_cook = cooks_distance > 1)

plot6 <- model_data |>
  ggplot(aes(x = index, y = cooks_distance)) +
  geom_point(shape = 1, color = "steelblue3") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "tomato") +
  labs(x = "Index", y = "Cook's Distance",
       title = "Cook's Distance") +
  theme_light() +
  theme(legend.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(size = 12, hjust = 0.5))


plot7 <- suppressMessages({
  model_data |>
  ggplot(aes(x = leverage, y = resids)) +
  geom_point(shape = 1, color = "steelblue3") +
  geom_smooth(method = "loess", color = "tomato", se = FALSE, span = 1) +
  labs(x = "Leverage", y = "Standardized Residuals",
       title = "Standardized Residuals vs Leverage") +
  theme_light() +
  theme(plot.title = element_text(size = 12, hjust = 0.5))
})



suppressMessages({
  combined_plot <- (plot1 + plot2) / (plot3 + plot4) / (plot5 + plot6) / (plot7 + plot_spacer()) +
  plot_annotation(title = expression(bold("Figure 13") * ": Diagnostic Plots"),
                  tag_levels = "A") &
    theme(plot.title = element_text(hjust = 0.5))
  print(combined_plot)
})
```

In Figure 13, Plot F shows a point with a higher Cook's distance than all the other points; this is house #89 which we identified as a potential high leverage point in the EDA section. However, since the Cook's distance value is less than our threshold of $1$, we do not classify it as a bad leverage point. If we look at Plot D, all the points are within the $[-3.3, 3.3]$ interval which we get after setting the confidence level to $\alpha / n$ where $\alpha = 0.05$. Hence, when we calculate the corresponding z-values they turn out to be $z_{\alpha/(2n)} = 3.3$ and $z_{-\alpha/(2n)} = -3.3$ respectively. All studentized residuals within these limits imply we do not have any outliers that are adversely affecting our model. Plot D also shows that even though the variance of the residuals is not exactly constant, it does seem fairly stable. Q-Q plot in Plot E supports normality of the residuals, there are a few points in the tails farther away from the normal line however - 1) this can be attributed to randomness and 2) almost all points are still within the global confidence interval band calculated using confidence level of $\alpha/n$ to cater for multiple testing. The house prices are assumed to be independent of each other which is appropriate since the price of one house usually does not have an effect on the price of any other house. Since the assumptions of linear regression seem to hold true, we shift our attention to answering our research question.

# Results

The estimated coefficients of `bedroom`, and `garage_spaces` do not have significant $p-values$. One plausible explanation for this is that the variation in the house prices explained by these variables are also captured by the `squarefeet` variable which has a highly significant $p-value$. From their corresponding plots with `squarefeet` in Figure 11, `bedrooms` and `garage_spaces` show a positive association with `squarefeet` and have correlation values of $0.703$ and $0.394$ respectively. `acre` neither shows a clear trend in the plot with `squarefeet` nor does it have a high correlation value with it but it still has an insignificant p-value. All three of these variables do not affect our findings about the `distance` variable and therefore we leave them in the model. However, it is pertinent to highlight that the coefficient of `acre` does show unexpected behavior which suggests there are certainly more variables not present in the data set that can be used to explain this anomaly.

The `distance` variable, our primary variable of concern, has an estimated coefficient of -0.05 with a $p-value\quad 0.009 < 0.05$ which leads us to reject the null hypothesis $H_0$ at the $5\%$ significance level, implying that for a one unit increase in the distance from the nearest rail trail entry, the average price of the house decreases by $0.05\%\; \pm\; 0.04\%\quad (p-value = 0.009<0.05)$ with 95% probability after taking into account the effect of all the other covariates. However, it is important to not confuse this association for causation since our data is collected from an observational study, not a randomized experiment.

# Conclusion

Our study found that homes closer to the rail trail are worth more. For every foot nearer to the trail, a home's value increases by about 0.05%. This means a house 1,000 feet closer could be priced around 5% higher than a similar house farther away. Larger homes also sell for more; every extra 1,000 square feet adds about 38.65% to a home's value.

For Acme Homes, this means building homes near rail trails can increase property values and profits. By adjusting home prices based on proximity to the trail, the company can sell closer homes at higher prices. Marketing the benefits of easy trail access—like walking and biking opportunities—can attract more buyers. Offering larger homes or options to expand can also boost sales. Homes near amenities like good schools, parks, and shops have higher values, so developing in areas with these features or adding community amenities can enhance property appeal.

While our findings are significant, they are based on homes smaller than 0.56 acres in two specific ZIP codes, so results may not apply to larger properties or other areas. Also, since this is an observational study, we cannot confirm that being closer to the rail trail causes higher prices—only that they are related. By applying these insights, Acme Homes can attract more buyers and increase profits by focusing on properties near rail trails and amenities.
